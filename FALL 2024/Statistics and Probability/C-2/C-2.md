  

**2.1: Sample space and Events:**

1. Some experiments don’t have that big of a random disturbance but some may have them large
2. A mathematical model abstraction is developed for a physical system. Combined with data from the system (it doesn’t have to be a perfect model) it becomes useful in analysis.
    
    ![[image 62.png|image 62.png]]
    

  

1. Most of the time, a system has noise variables or disturbance, basically inputs not being controlled by us. In this case, a system will not give us identical results even if we use identical inputs every time because of the noise.
    
    ![[image 1 14.png|image 1 14.png]]
    

  

1. An experiment that can result in different outcomes, even though it is repeated in the same manner every time, is called a **random experiment.**

  

1. To analyze a random experiment, we must know the set of all possible outcomes of the experiment. This set is called the **Sample space** of the experiment denoted by _**S.**_

  

1. This S is defined based on the objectives of the analysis and may differ accordingly.

  

1. A sample space is **discrete** if it consists of a finite or countable infinite set of outcomes. Not intervals but individual, countable units. {a, b, c…}

  

1. A sample space is **continuous** if it contains an interval (either finite or infinite) of real numbers. [2, inf)

  

1. When a sample space can be constructed in several steps or stages, we can represent each of the n1 ways of completing the first step as a branch of a tree. Each of the ways of completing the second step can be represented as n2 branches starting from the ends of the original branches, and so forth.

![[c02f005.jpg]]

Tree diagram for the sample space of 3 messages arriving either late or on time.

  

1. **Event** is a subset of the sample space that we might be interested in.

  

1. We can also use set operations to build other events of interests.

  

1. Some examples:
    
    - Union of two events is the event that consists of all outcomes that are contained in either of the two events. $E_1 U E_2$
    - Intersection of 2 events is the event that consists of all outcomes that are present in both of the two events. $E_1 \cap E_2$
    - Complement of an event is the set of outcomes in S that are not in the event. We denote the complement of the event as $E’$ or $E^c$.
    
      
    
2. Two Events, E1 and E2 such that,
    
    E₁ ∩ E₂ = ∅, are said to be **mutually exclusive.**
    
      
    
3. **Mutually exclusive events:** A collection of events whose intersections are empty.

  

1. Distributive, Associative and De morgan’s laws are applicable.

  

  

## 2.2: Counting Techniques

  

1. **Counting techniques:** Formulas used to determine the number of elements in sample spaces and events.
2. _**Multiplication Rule (for counting techniques):**_  
    Assume an operation can be described as a sequence of k steps, and  
    the number of ways to complete step 1 is n1, and  
    the number of ways to complete step 2 is n2 for each way to complete step 1, and  
    the number of ways to complete step 3 is n3 for each way to complete step 2, and  
    so forth.  
    The total number of ways to complete the operation is  
    
    n1 * n2 * n3 …… * nk
    
      
    
      
    
3. **Permutation:**

An ordered sequence of the elements in a set used to determine the number of outcomes in events and sample spaces.

==**How many different combinations of n elements is there:**==

==$n!$==

==**This n! comes from the multiplication rule only as:**==

==$a_0, a_1, a_2,….a_n$== ==**is a combination of n terms**==

==**the possibilities for the first term,**== ==$p_0 = n$==

==**the possibilities for the second term,**== ==$p_1 = n-1$==

==**the possibilities for the 3rd term,**== ==$p_2 = n-2 ….$== ==**and so on**==

==**ultimately the total number of possibilities for the whole combination become:**==

==$p_0 * p_1 * p_2 ……p_n = n * (n-1) * (n-2) * ….. * 2 * 1 = n !$==

  

  

1. When we are interested in the arrangement of only some of the elements of the set instead of all of them,
    
      
    
    > ==**Let r be the number of elements we are interested in and let n be the number of elements in the set.**==
    
    ==**then permutations:**==
    
    > ==$P^n_r = n * (n-1) * (n-2) * …. * (n-r+1)$==
    
      
    
    > ==**=**== ==$n!/(n-r)!$==
    
    > The above is proved by the following equality :
    
    > ==$n! = n * (n-1) * (n-2) * (n - r + 1) * (n-r)!$==
    
    > ==**this is because n-r is the number of elements the set has an excess of from r and the 1 is just the one element remaining for r.**==
    
      
    

  

1. When we have objects that are not all different and we want to calculate no. of permutations, (so basically, if one permutation occurs twice, we count only one of them).
    
    > ==**let’s suppose we have 5 elements, {**====$h, h , a, a, a$====**}**==
    
    > ==**there can be 5! permutations if we consider the 2 hs and 3 as as distinct elements and if we look at it in a different way: {**====$h_1, h_2, a_1, a_2, a_3$====**}. So basically there are 5! permutations but in those 5! permutations, each permutation has been repeated**== ==$2! * 3!$== ==**times, (number of ways**== ==$h_1, h_2, a_1, a_2, a_3$== ==**can be arranged) so the number of distinct permutation is**== ==$5!/(2!*3!)$====**.**==
    
    > ==**= 10.**==
    
    > ==**In general, if we have n number of elements where n =**== ==$n1 + n2 + …. n_r$== ==**and n1 are of one type, n2 are of another and so on, the number of distinct permutations =**== ==$n!/(n1!*n2!*….nr!)$====.==
    

  

1. **Combination:** A **subset** selected without replacement from a set used to determine the number of outcomes in events and sample spaces.
    
    number of subsets of r elements that can be selected from a set of n elements.
    
    Basically, Combination is the number of r-sets in a n element set, when order is not important, i.e, {a,b,c} and {b,a,c} must count as one since they have the same elements. (r-permutations of a set of n elements is different and counts different orders of same elements).
    
    We call n choose r :
    
    $C^n_r = (^n_r) = n!/(r!(n-r)!)$To account for the fact that each permutation has r! different permutations with the same elements.
    
      
    
2. In random experiments in which items are selected from a batch, an item may or may not be replaced before the next one is selected. This is referred to as sampling **with** or **without replacement**, respectively.

  

  

  

## 2.3 Interpretations and Axioms of Probability:

1. **Probability:** A numerical measure between 0 and 1 assigned to events in a sample space. Higher numbers indicate the event is more likely to occur. See Axioms of probability. A 0 indicates an outcome will not occur. A probability of 1 indicates that an outcome will occur with certainty.

  

1. Relative frequency of corrupted pulses sent over a communication channel.

![[c02f008.jpg]]

  

1. Probability is a proportion of replications that result in the outcome with total number of replications. This proportion can be interpreted as a Relative Frequency like above.

  

1. There are multiple models to assign probabilities:
    
    - When the events are all equally likely to happend, the probabilites are all equal.
    
      
    
    - Whenever a sample space consists of N possible outcomes that are equally likely, the probability of each outcome is 1/N.
    
      
    
2. Sometimes, an event involves multiple outcomes. In this case, Probability of the Event is the sum of all the probabilities of those outcomes.
    
    > For a discrete sample space, the probability of an event E, denoted as P(E), equals the sum of the probabilities of the outcomes in E.
    

  

  

1. AXIOMS OF PROBABILITIES:
    - P(S) = 1 where S is sample space
    - 0 ≤ P(E) ≤ 1 for any event E
    - For 2 events E1 and E2 with E1∩E2 = ∅, P(E1 ∪ E2) = P(E1) + P(E2)
    - This axiom extends to any finite number of mutually exclusive events: P(E1 ∪ E2 ∪ ... ∪ En) = P(E1) + P(E2) + ... + P(En)

  

1. If an Event A is contained in another event B,
    
    $P(A) ≤ P(B)$
    

  

  

## 2.4 Unions of Events and Addition Rules:

1. Probability of a joint event can be determined from the individual probabilities of the events being joined
2. Addition Rule:
    
    Probability of a union:
    
    > **P(A ∪ B) = P(A) + P(B) - P(A ∩ B)**
    

  

1. Events are mutually exclusive if they don’t over lap (not intersection)

  

1. For a collection of mutually exclusive Events, E1, E2, ….. Ek:
    
    P(E1 U E2 U …. U E_k) =
    
    $P(E1) + P(E2) + ... + P(E_k)$
    
    This is because mutually exclusive events have no overlap, so we can simply sum their individual probabilities. This rule is particularly useful when dealing with complex systems or multiple scenarios that cannot occur simultaneously.
    

  

## 2.5 Conditional Probability:

  

1. Conditional probability: The probability of an event given that the random experiment produces an outcome in another event.

Best example:

> [!important] In a manufacturing process, 10% of the parts contain visible surface flaws and 25% of the parts with surface flaws are (functionally) defective parts. However, only 5% of parts without surface flaws are defective parts. The probability of a defective part depends on our knowledge of the presence or absence of a surface flaw. Let D denote the event that a part is defective, and let F denote the event that a part has a surface flaw. Then we denote the probability of D given or assuming that a part has a surface flaw, as P(D | F). Because 25% of the parts with surface flaws are defective, our conclusion can be stated as P(D | F) = 0.25. Furthermore, because F′ denotes the event that a part does not have a surface flaw and because 5% of the parts without surface flaws are defective, we have P(D | F′) = 0.05.

  

1. We can understand this concept like this:
    
    P(A) = Probability of event A =
    
    no. of outcomes in A / N
    
    P(B) = Probability of event B =
    
    no. of outcomes in B / N , N = Total outcomes.
    
      
    
    Now P(B|A) assumes that random experiment yield an outcome that is in the event A (event A has happened) and now depending on this we have to determine probability that this outcome is also a part of Event B or, the probability that Event B is also happening.
    
    This is given by,
    
    $P(B|A) = P(A ∩ B)/(P(A))$
    
    > Probability of event B happening given Probablity of event A. The equation above says, how many outcomes are there that both B and A have in them and Sample space becomes the probability that A will happen (out of that, how many are in the intersection of B and A).
    
    This probability depends on another probability
    

  

1. Therefore, P(B | A) can be interpreted as the relative frequency of event B among the trials that produce an outcome in event A.
    
    ![[c02f012.jpg]]
    

  

  

## 2.6 Intersections of Events and Multiplication and Total Probability Rules:

1. Multiplication rule:
    
    P(A ∩ B) = P(A) * P(B|A) = P(A|B) * P(B)
    

  

1. We have the following derivation:
    
    $P(B) = P(B \cap A) + P(B \cap A’) = P(B|A)*P(A) + P(B|A’)P(A’)$
    
    ![[image 2 12.png|image 2 12.png]]
    

  

  

  

## 2.7 Independence

1. In the special case where event A does not affect out comes in event B,
    
    P(B|A) = P(B).
    
    this derives the following:
    
    $P(A \cap B) = P(B|A)(P(A)$
    
    $⇒ P(A \cap B) = P(B)P(A)$
    
    $⇒ P(A|B) = P(A)$
    
      
    

  

1. **Independence: Two events are independent if any one of the following statements are true:**
    - $P(A|B) = P(A)$
    - $P(B|A) = P(B)$
    - $P(A \cap B) = P(A)P(B)$

![[image 3 12.png|image 3 12.png]]

This definition is typically used to calculate the probability that several events occur, assuming that they are independent and the individual event probabilities are known. The knowledge that the events are independent usually comes from a fundamental understanding of the random experiment.

  

  

## 2.8: Baye’s theorem:

Most of the times we want to know the probability of some condition event A knowing an outcome event B (going backwards) For this Baye’s theorem is useful:

  

$P(A \cap B) = P(A|B)P(B) = P(B \cap A) = P(B|A)P(A)$

  

Using the second and the last term,

$⇒ P(A|B) = P(B|A)P(A)/P(B) \ for\ P(B) > 0$

  

We basically went backwards and derived P(A|B) in terms of P(B|A)

  

> [!important] General Baye’s theorem:
> 
> If E1, E2, E3…Ek are mutually exclusive and exhaustive events (Pairs of E and E’) and B is any event,
> 
>   
> 
> $P(E_1|B) = P(B|E_1)P(E_1)/[P(B|E_1)P(E_1) + …. P(B|E_k)P(E_k)]$ …..(using total probability rule)

  

  

  

## 2.9: Random Variables:

1. Random Variable: A random variable is a function that assigns a real number to each outcome in the sample space of a random experiment. Basicall when we don’t know the actual outcome of the random experiment, we use these random variables.

  

1. Notation: A random variable is denoted by an uppercase letter such as X. After an experiment is conducted, the measured value of the random variable is denoted by a lowercase letter such as x = 70 milliamperes.

  

1. Continuous random variable: A random variable with an interval (either finite or infinite) of real numbers for its range.

  

> [!important] Examples of Random Variables:
> 
>   
> Examples of continuous random variables:  
> 
> electrical current, length, pressure, temperature, time, voltage, weight
> 
> Examples of discrete random variables:
> 
> number of scratches on a surface, proportion of defective parts among 1000 tested, number of transmitted bits received in error